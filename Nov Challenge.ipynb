{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novartis Data Challenge\n",
    "Author: Eli Goldberg\n",
    "\n",
    "Language: Python 3.5+\n",
    "    \n",
    "#### Disclaimer\n",
    "2-4 hours seemed quite short for what I wanted to do, without recycling functions from elsewhere. However, I will say that everything employed here was coded within the given timeperiod. To keep things simple, I stuck to using the accelerometery data. There are other methods to employ this type of data, but given the time limit, it seemed reasonable to limit the analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.ticker as ticker\n",
    "mpl.rcParams['figure.figsize'] = (12,5)\n",
    "from matplotlib import colors as mcolors\n",
    "colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "import numpy as np\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define peakdetect from script - no time to mess with walking through setup.py\n",
    "import sys\n",
    "from numpy import NaN, Inf, arange, isscalar, asarray, array\n",
    "\n",
    "def peakdet(v, delta, x=None):\n",
    "    \"\"\"\n",
    "    Converted from MATLAB script at http://billauer.co.il/peakdet.html\n",
    "\n",
    "    Returns two arrays\n",
    "\n",
    "    function [maxtab, mintab]=peakdet(v, delta, x)\n",
    "    %PEAKDET Detect peaks in a vector\n",
    "    %        [MAXTAB, MINTAB] = PEAKDET(V, DELTA) finds the local\n",
    "    %        maxima and minima (\"peaks\") in the vector V.\n",
    "    %        MAXTAB and MINTAB consists of two columns. Column 1\n",
    "    %        contains indices in V, and column 2 the found values.\n",
    "    %      \n",
    "    %        With [MAXTAB, MINTAB] = PEAKDET(V, DELTA, X) the indices\n",
    "    %        in MAXTAB and MINTAB are replaced with the corresponding\n",
    "    %        X-values.\n",
    "    %\n",
    "    %        A point is considered a maximum peak if it has the maximal\n",
    "    %        value, and was preceded (to the left) by a value lower by\n",
    "    %        DELTA.\n",
    "\n",
    "    % Eli Billauer, 3.4.05 (Explicitly not copyrighted).\n",
    "    % This function is released to the public domain; Any use is allowed.\n",
    "\n",
    "    \"\"\"\n",
    "    maxtab = []\n",
    "    mintab = []\n",
    "\n",
    "    if x is None:\n",
    "        x = arange(len(v))\n",
    "\n",
    "    v = asarray(v)\n",
    "\n",
    "    if len(v) != len(x):\n",
    "        sys.exit('Input vectors v and x must have same length')\n",
    "\n",
    "    if not isscalar(delta):\n",
    "        sys.exit('Input argument delta must be a scalar')\n",
    "\n",
    "    if delta <= 0:\n",
    "        sys.exit('Input argument delta must be positive')\n",
    "\n",
    "    mn, mx = Inf, -Inf\n",
    "    mnpos, mxpos = NaN, NaN\n",
    "\n",
    "    lookformax = True\n",
    "\n",
    "    for i in arange(len(v)):\n",
    "        this = v[i]\n",
    "        if this > mx:\n",
    "            mx = this\n",
    "            mxpos = x[i]\n",
    "        if this < mn:\n",
    "            mn = this\n",
    "            mnpos = x[i]\n",
    "\n",
    "        if lookformax:\n",
    "            if this < mx - delta:\n",
    "                maxtab.append((mxpos, mx))\n",
    "                mn = this\n",
    "                mnpos = x[i]\n",
    "                lookformax = False\n",
    "        else:\n",
    "            if this > mn + delta:\n",
    "                mintab.append((mnpos, mn))\n",
    "                mx = this\n",
    "                mxpos = x[i]\n",
    "                lookformax = True\n",
    "\n",
    "    return array(maxtab), array(mintab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Read in data\n",
    "* I used pandas to read the shimmer file .csv into a dataframe, knowing that the delimiter is ';'. \n",
    "\n",
    "* The headers were long and overly descriptive, so I changed them to something simpler. In addition, the units are contained in the second row of the file, so I filtered that out in a rather inelegant, but effective, fashion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          t_ms        ax_cal        ay_cal        az_cal     ax_acl_wr  \\\n",
      "1  979.2480469  -0.204819277  -9.253012048  -0.554216867  -0.592592593   \n",
      "2  989.0136719  -0.168674699  -9.542168675  -0.927710843  -0.592592593   \n",
      "3  998.7792969  -0.072289157  -9.771084337  -1.481927711  -0.711111111   \n",
      "4  1008.544922  -0.072289157  -9.915662651  -1.421686747  -0.355555556   \n",
      "5  1018.310547  -0.120481928  -9.903614458  -1.253012048   -0.82962963   \n",
      "\n",
      "      ay_acl_wr     az_acl_wr       gx_cal       gy_cal       gz_cal  \\\n",
      "1  -9.362962963  -0.948148148  6.183206107  0.229007634  3.938931298   \n",
      "2  -9.362962963  -0.711111111   3.58778626  1.221374046  1.267175573   \n",
      "3  -9.244444444  -1.066666667  5.160305344  1.083969466  1.435114504   \n",
      "4  -9.481481481  -1.066666667  14.32061069  1.740458015  0.916030534   \n",
      "5  -9.718518519   -0.82962963  25.49618321  2.183206107  0.351145038   \n",
      "\n",
      "         mx_cal       my_cal       mz_cal  \n",
      "1   -0.49005848  0.223391813  0.046052632  \n",
      "2   -0.49005848  0.223391813  0.042105263  \n",
      "3   -0.49005848  0.222222222  0.046052632  \n",
      "4  -0.487719298  0.221052632  0.043421053  \n",
      "5   -0.49122807  0.225730994         0.05  \n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "data = pd.read_csv('shimmer.csv', delimiter=';', low_memory=False)\n",
    "\n",
    "# define array of simplified headers\n",
    "new_headers = ['t_ms', 'ax_cal', 'ay_cal', 'az_cal',\n",
    "               'ax_acl_wr', 'ay_acl_wr', 'az_acl_wr',\n",
    "               'gx_cal', 'gy_cal', 'gz_cal', 'mx_cal',\n",
    "               'my_cal', 'mz_cal']\n",
    "\n",
    "# replace the headers\n",
    "data = pd.DataFrame(data.as_matrix(), columns=new_headers)\n",
    "\n",
    "# remove the units row\n",
    "data = data[1:]\n",
    "\n",
    "# print the first 5 rows to check\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Combining features to create more interpretable data\n",
    "Working with raw acceleration and magnetometry data can be useful, but it's often helpful to combine these data to make more interpretable features, like the total acceleration magnitude, and pitch and roll (if you don't care about direction, which we don't for a balance exercise).\n",
    "\n",
    "It can be slow to manually apply data to large matrices, so I've created some simple functions to be used with pandas' apply functionality for their dataframes. \n",
    "* acc2totalMag: total acceleration magnitude should help us determine shaking from not shaking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acc2totalMag(row):\n",
    "    Ax = float(row['ax_cal'])\n",
    "    Ay = float(row['ay_cal'])\n",
    "    Az = float(row['az_cal'])\n",
    "    return math.sqrt(Ax ** 2 + Ay ** 2 + Az ** 2)\n",
    "\n",
    "def acc2pitch(row):\n",
    "    ''' https://stanford.edu/class/ee267/lectures/lecture10.pdf\n",
    "    assumes y-axis is up'''\n",
    "    Ax = float(row['ax_cal'])\n",
    "    Ay = float(row['ay_cal'])\n",
    "    Az = float(row['az_cal'])\n",
    "    return math.degrees(-math.atan2(Az, math.sqrt(Ax ** 2 + Ay ** 2)))\n",
    "\n",
    "def acc2roll(row):\n",
    "    ''' https://stanford.edu/class/ee267/lectures/lecture10.pdf\n",
    "    assumes y-axis is up'''\n",
    "    Ax = float(row['ax_cal'])\n",
    "    Ay = float(row['ay_cal'])\n",
    "    Az = float(row['az_cal'])\n",
    "    return math.degrees(-math.atan2(-Ax, Ay))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply the functions to the data. \n",
    "data['totalMag'] = data.apply(acc2totalMag, axis=1)\n",
    "data['pitch'] = data.apply(acc2pitch, axis=1)\n",
    "data['roll'] = data.apply(acc2roll, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E) Determining start/stop events for balance test \n",
    "My first inclination to determine the start/stop times for the balance test is to look at the magnitude of total acceleration. As given in the prompt, at the start and end of each balance test, the subject was asked to shake the sensor vigorously. Also, each balance test lasted for a duration of 40 seconds! A completely autonomous signal analysis isn't super trivial to do, but i'll give it my best in the remaining time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the signal is noisy, so introduce a quick weighted moving ave filter\n",
    "def weighted_moving_average(signal=[], weights=[]):\n",
    "    ''' use a quick convolution to smooth the signal'''\n",
    "    w_signal = np.convolve(signal, np.array(weights)[::-1], mode='valid')\n",
    "    return w_signal / np.sum(weights)\n",
    "\n",
    "time = data['t_ms']\n",
    "sig_totMag = data['totalMag']\n",
    "window_size = 200\n",
    "weights = np.hamming(window_size)\n",
    "sig_totMag_weig_filt = weighted_moving_average(signal=sig_totMag,\n",
    "                                               weights=weights)\n",
    "\n",
    "# Two subplots, plot the pitch and roll\n",
    "f, axarr = plt.subplots(2, sharex=True)\n",
    "axarr[0].set_title('Raw Total Acc. Mag. (m2/s)')\n",
    "axarr[0].plot(sig_totMag)\n",
    "axarr[0].set_ylabel('Total Acc. Mag. (m2/s) - unfilt.')\n",
    "axarr[1].set_title('Filtered Total Acc. Mag. (m2/s)')\n",
    "axarr[1].plot(sig_totMag_weig_filt)\n",
    "axarr[1].set_ylabel('Total Acc. Mag. (m2/s)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.1) Unsupervised identification of start/stops\n",
    "The above is likely ok for visual identification, but I think we can make a fairly simple unsupervised method. I'm starting to run low on time, so I'll try two easy classification schemes. The idea is to identify the start/stop periods, and use them to create seperate data files. This may result in an imperfect classification, but hey - I've only got limited time. \n",
    "\n",
    "* Classification by coefficient of variation (CV) for a moving window\n",
    "* Classification by total signal energy deviation from mean for a moving window\n",
    "* Peak identification with some post processing \n",
    "* Supervised ML-based classification with moving window (requires supervised data, which we could technically make)\n",
    "* Clustering and PCA - we could project the singnals down to a few axes and see what pops up. This could also work well\n",
    "\n",
    "There are a lot of things tht could be done, but it's a bit too much guess work and not enough time. I'll pick a method that's a bit more supervised, but effective given the time period - I hope. I'm likely wasting precious time writing commentary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.2) Peak detection with whole, and then subset, of the series\n",
    "Well, I tried to do some peak detection of the entire series. It worked ok. Because the vigor of the shaking was not equal in each start/stop, you get some over-identification or under identification of peaks. It may be best to split the data up data into users at this point. This is only the first question though, so I'm not going to go crazy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theoretical total number of tests\n",
    "subjects = 3\n",
    "tests_per_subject = 8\n",
    "on_offs_per_test = 2\n",
    "total_on_ofs = subjects * tests_per_subject * on_offs_per_test\n",
    "print(total_on_ofs, 'theoretical # of peaks')\n",
    "\n",
    "# perhaps more signal shaping is needed - bring this into this cell\n",
    "window_size = 50\n",
    "weights = np.bartlett(window_size)\n",
    "# weights = np.kaiser(window_size,5)\n",
    "weights = np.ones(window_size)\n",
    "sig_totMag_weig_filt = weighted_moving_average(signal=sig_totMag,\n",
    "                                               weights=weights)\n",
    "\n",
    "#use the peakdet function introduced above\n",
    "maxtab, mintab = peakdet(sig_totMag_weig_filt, 4)\n",
    "print(len((maxtab)[:, 0]), 'detected # of peaks')\n",
    "plt.plot(sig_totMag_weig_filt)\n",
    "plt.scatter(array(maxtab)[:, 0], array(maxtab)[:, 1], color='blue')\n",
    "plt.scatter(array(mintab)[:, 0], array(mintab)[:, 1], color='red')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.3) Isolation of a single subject's data by start and stop time\n",
    "Using the peak-assist data indices, I split the data into segments by visual inspection (i.e., counting the blue dots and figuring out which ones were in between data. I'd rather do it in a completely unsupervisd manner, but that's not possible with this short of time. Note: if you run this and then try to run it again, it'll break. To fix, re-run the previous cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual ID using peak-assisted indexes. \n",
    "# subject 1: peak ids for start/stop = 1, 18\n",
    "subj_1_index_start = int(array(maxtab)[1, 0])\n",
    "subj_1_index_stop = int(array(maxtab)[18, 0])\n",
    "print(subj_1_index_start, subj_1_index_stop)\n",
    "\n",
    "# subject 2\n",
    "# peak ids for start/stop = 19, 35\n",
    "subj_2_index_start = int(array(maxtab)[18, 0])\n",
    "subj_2_index_stop = int(array(maxtab)[35, 0])\n",
    "print(subj_2_index_start, subj_2_index_stop)\n",
    "\n",
    "# subject 2\n",
    "# peak ids for start/stop = 36, 56\n",
    "subj_3_index_start = int(array(maxtab)[36, 0])\n",
    "subj_3_index_stop = int(array(maxtab)[56, 0])\n",
    "print(subj_3_index_start, subj_3_index_stop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.4) Plotting and delineating start/stops for subject 1. \n",
    "Isolating a single subjects data allows us to make the peak detection function introduced above a bit more sensitive. It turns out that it wasn't needed for subject one. However, it's suggested that the first and last 5 seconds for each data should be discarded (5 seconds = 5s*102.4/s = 512 data points on each end). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for subject 1\n",
    "#use the peakdet function introduced above\n",
    "maxtab, mintab = peakdet(sig_totMag_weig_filt[subj_1_index_start:subj_1_index_stop], 3)\n",
    "print(len((maxtab)[:, 0]), 'detected # of peaks')\n",
    "plt.plot(sig_totMag_weig_filt[subj_1_index_start:subj_1_index_stop])\n",
    "plt.scatter(array(maxtab)[:, 0], array(maxtab)[:, 1], color='blue')\n",
    "plt.scatter(array(mintab)[:, 0], array(mintab)[:, 1], color='red')\n",
    "plt.show()\n",
    "\n",
    "# remove a bit of the front and back, because the first/last part of each run is garbage. \n",
    "# then grab the indexes for the data in between the peaks\n",
    "# start data \n",
    "sub1_start = (maxtab)[:, 0][1::2] + 512 + subj_1_index_start\n",
    "sub1_stop = (maxtab)[:, 0][2::2] - 512 + subj_1_index_start\n",
    "print(sub1_start)\n",
    "print(sub1_stop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.5) Plotting and delineating start/stops for subject 2. \n",
    "I increased the peak thresholding to remove the errors detected in subject two in test 8. Without better notes, it's not clear to me if test 8 ended at the last peak, or if that was the reset. As such, I'm tempted to keep my pure list comprehesion method for extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for subject 2\n",
    "#use the peakdet function introduced above\n",
    "maxtab, mintab = peakdet(sig_totMag_weig_filt[subj_2_index_start:subj_2_index_stop], 5)\n",
    "print(len((maxtab)[:, 0]), 'detected # of peaks')\n",
    "plt.plot(sig_totMag_weig_filt[subj_2_index_start:subj_2_index_stop])\n",
    "plt.scatter(array(maxtab)[:, 0], array(maxtab)[:, 1], color='blue')\n",
    "plt.scatter(array(mintab)[:, 0], array(mintab)[:, 1], color='red')\n",
    "plt.show()\n",
    "\n",
    "# remove a bit of the front and back, because the first/last part of each run is garbage. \n",
    "# then grab the indexes for the data in between the peaks\n",
    "# start data starts at 0 index for maxtab starts,and 1 for maxtab stops\n",
    "sub2_start = (maxtab)[:, 0][::2] + 512 + subj_2_index_start\n",
    "sub2_stop = (maxtab)[:, 0][1::2] - 512 + subj_2_index_start\n",
    "print(sub2_start)\n",
    "print(sub2_stop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.6) Plotting and delineating start/stops for subject 3. \n",
    "Please note that typically, I would do this with a clever loop. However, this game seems to be about speed. Also, I do see some problems in using pure list comprehension to specify the starts/stops for subject 3, as the early failure in the 6th test caused a restart. I'll threshold as much as possible needed and then create the indexes specifically, if needed. Turns out a threshold of 5.3 for the peakdet worked well, but test 7 only has one start point. Therefore I created seperate vectors with the peak number (0-14), so I could relate that back to the index. Could be more efficient. You can see the fail on test 6 clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for subject 3\n",
    "#use the peakdet function introduced above\n",
    "maxtab, mintab = peakdet(sig_totMag_weig_filt[subj_3_index_start:subj_3_index_stop], 5.3)\n",
    "print(len((maxtab)[:, 0]), 'detected # of peaks')\n",
    "plt.plot(sig_totMag_weig_filt[subj_3_index_start:subj_3_index_stop])\n",
    "plt.scatter(array(maxtab)[:, 0], array(maxtab)[:, 1], color='blue')\n",
    "plt.scatter(array(mintab)[:, 0], array(mintab)[:, 1], color='red')\n",
    "plt.show()\n",
    "\n",
    "# create vectors of start/stop indices into new arrays\n",
    "select_start_index = [0, 2, 4, 6, 8, 10, 11, 13]\n",
    "select_stop_index = [1, 3, 5, 7, 9, 11, 12, 14]\n",
    "\n",
    "# initialize bins for indices\n",
    "sub3_start = []\n",
    "sub3_stop = []\n",
    "\n",
    "# loop through selected peaks and pull out indices for each\n",
    "for i in range(0, len(select_start_index)):\n",
    "    sub3_start.append((maxtab)[select_start_index[i], 0] + 512 + subj_3_index_start)\n",
    "    sub3_stop.append((maxtab)[select_stop_index[i], 0] - 512+ subj_3_index_start)\n",
    "\n",
    "print(sub3_start)\n",
    "print(sub3_stop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F) visualizing the pitch and roll for the entire time series\n",
    "Now that we've got a semi-autonomous method to isolate each subject's signal by inspection of the total magnitude of acceleration, let's take a look at it. \n",
    "\n",
    "As indicated in the prompt, the sensors were occasionally oriented such that the typical 'up' direction, the y-axis, was perpendicular to gravity. However, looking at the roll, it's likely that the sensors were perpendicular to gravity during this period. I also scaled the x-axis to reflect time and not device cycles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted moving characteristics\n",
    "window_size = 200\n",
    "weights = np.blackman(window_size)\n",
    "# weights = np.ones(window_size)  # equal to a moving average\n",
    "\n",
    "sig_roll = data['roll']\n",
    "sig_pitch = data['pitch']\n",
    "sig_totMag = data['totalMag']\n",
    "time = data['t_ms']\n",
    "\n",
    "# apply weighted moving average function to smooth signal data\n",
    "sig_roll_weig_filt = weighted_moving_average(signal=sig_roll,\n",
    "                                             weights=weights)\n",
    "sig_pitch_weig_filt = weighted_moving_average(signal=sig_pitch,\n",
    "                                              weights=weights)\n",
    "sig_totMag_weig_filt = weighted_moving_average(signal=sig_totMag,\n",
    "                                               weights=weights)\n",
    "\n",
    "scale_x = 102.4  # hz\n",
    "# plot the pitch, roll, and total magnitude\n",
    "f, axarr = plt.subplots(2, sharex=True)\n",
    "axarr[0].plot(sig_pitch_weig_filt)\n",
    "axarr[0].set_ylabel('pitch (deg.)')\n",
    "axarr[0].set_ylim(-90, 90)\n",
    "axarr[0].set_title('Sharing X axis')\n",
    "axarr[1].plot(sig_roll_weig_filt)\n",
    "axarr[1].set_ylabel('roll (deg.)')\n",
    "axarr[1].set_ylim(-200, 200)\n",
    "ticks = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x / scale_x))\n",
    "axarr[0].xaxis.set_major_formatter(ticks)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F) Balance score metrics\n",
    "There are likley quite a few metrics that would be useful in scoring balance exercise. I can think of two easy measures of global activity and stability. The first is looking at the total signal energy deviation from the mean. The lower the signal energy, the better. The second test would be a measuring the test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F.1) Balance metrics for subject 1\n",
    "Although the plots aren't as clean as I normally like, visualizing the pitch angle and the total magnitude of the accelerometers give an interesting take on the difficulty. I do notice a few discrepencies, like in test 3, where the acceleromter data is low, although the pitch angle swings widely. It is, of course, posisble, but would need to be investigated further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 4)\n",
    "fig.subplots_adjust(hspace=.5, wspace=.5)\n",
    "axs = axs.ravel()\n",
    "\n",
    "i = 1\n",
    "for start, stop in zip(sub1_start, sub1_stop):\n",
    "    axs[i - 1].plot(sig_pitch_weig_filt[int(start):int(stop)],color='mediumblue')\n",
    "    axs[i - 1].twinx().plot(sig_totMag_weig_filt[int(start):int(stop)],color='orange')\n",
    "    axs[i - 1].set_title('test: ' + str(i))\n",
    "    axs[i - 1].set_ylim(-20, 60)\n",
    "\n",
    "    i = i + 1\n",
    "axs[0].set_ylabel('Pitch (deg.) & Acc Mag(m^2/s)')\n",
    "axs[4].set_ylabel('Pitch (deg.) & Acc Mag(m^2/s)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_test_track = []\n",
    "sigDevEn_test_track = []\n",
    "\n",
    "cv_test_track_acc = []\n",
    "sigDevEn_test_acc_track = []\n",
    "for start, stop in zip(sub1_start, sub1_stop):\n",
    "    # for each test \n",
    "    # look at coefficient of variation - lower is better\n",
    "    # look at signal energy - lower is better\n",
    "    test_data = sig_pitch_weig_filt[int(start):int(stop)]\n",
    "    test_data_acc = sig_totMag_weig_filt[int(start):int(stop)]\n",
    "\n",
    "    cv_test_data = np.std(test_data) / np.mean(test_data)\n",
    "    sigDevEn_test_data = np.sum((test_data - np.mean(test_data)) ** 2) / len(test_data)\n",
    "\n",
    "    cv_test_track.append(cv_test_data)\n",
    "    sigDevEn_test_track.append(sigDevEn_test_data)\n",
    "\n",
    "    cv_test_data_acc = np.std(test_data_acc) / np.mean(test_data_acc)\n",
    "    sigDevEn_test_data_acc = np.sum((test_data_acc - np.mean(test_data_acc)) ** 2) / len(test_data_acc)\n",
    "\n",
    "    cv_test_track_acc.append(cv_test_data_acc)\n",
    "    sigDevEn_test_acc_track.append(sigDevEn_test_data_acc)\n",
    "\n",
    "subj_1_data = np.vstack((sigDevEn_test_track, cv_test_track, \n",
    "                         cv_test_track_acc, \n",
    "                         sigDevEn_test_acc_track)).T\n",
    "\n",
    "subj_1_test_results = pd.DataFrame(subj_1_data,\n",
    "                                   columns=['sigDevEn_test_pitch', \n",
    "                                            'cv_test_pitch', \n",
    "                                            'sigDevEn_test_acc', 'cv_test_acc'])\n",
    "subj_1_meta_data = [0] * 8\n",
    "subj_1_test_results['Metadata Results'] = subj_1_meta_data\n",
    "\n",
    "print(subj_1_test_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F.2) Balance metrics for subject 2\n",
    "The balance metrics for test 8 on subject 2 look a bit strange, considering I see peaks in the data. I think I may have inadverently cuttoff the problem in subject 8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 4)\n",
    "fig.subplots_adjust(hspace=.5, wspace=.5)\n",
    "\n",
    "axs = axs.ravel()\n",
    "i = 1\n",
    "\n",
    "for start, stop in zip(sub2_start, sub2_stop):\n",
    "    axs[i - 1].plot(sig_pitch_weig_filt[int(start):int(stop)],color='mediumblue')\n",
    "    axs[i - 1].twinx().plot(sig_totMag_weig_filt[int(start):int(stop)],color='orange')\n",
    "    axs[i - 1].set_title('test: ' + str(i))\n",
    "    axs[i - 1].set_ylim(-20, 60)\n",
    "    i = i + 1\n",
    "\n",
    "axs[0].set_ylabel('Pitch (deg.) & Acc Mag(m^2/s)')\n",
    "axs[4].set_ylabel('Pitch (deg.) & Acc Mag(m^2/s)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_test_track = []\n",
    "sigDevEn_test_track = []\n",
    "\n",
    "cv_test_track_acc = []\n",
    "sigDevEn_test_acc_track = []\n",
    "for start, stop in zip(sub2_start, sub2_stop):\n",
    "    # for each test \n",
    "    # look at coefficient of variation - lower is better\n",
    "    # look at signal energy - lower is better\n",
    "    test_data = sig_pitch_weig_filt[int(start):int(stop)]\n",
    "    test_data_acc = sig_totMag_weig_filt[int(start):int(stop)]\n",
    "\n",
    "    cv_test_data = np.std(test_data) / np.mean(test_data)\n",
    "    sigDevEn_test_data = np.sum((test_data - np.mean(test_data)) ** 2) / len(test_data)\n",
    "\n",
    "    cv_test_track.append(cv_test_data)\n",
    "    sigDevEn_test_track.append(sigDevEn_test_data)\n",
    "\n",
    "    cv_test_data_acc = np.std(test_data_acc) / np.mean(test_data_acc)\n",
    "    sigDevEn_test_data_acc = np.sum((test_data_acc - np.mean(test_data_acc)) ** 2) / len(test_data_acc)\n",
    "\n",
    "    cv_test_track_acc.append(cv_test_data_acc)\n",
    "    sigDevEn_test_acc_track.append(sigDevEn_test_data_acc)\n",
    "\n",
    "subj_2_data = np.vstack((sigDevEn_test_track, cv_test_track, cv_test_track_acc, sigDevEn_test_acc_track)).T\n",
    "subj_2_test_results = pd.DataFrame(subj_2_data,\n",
    "                                   columns=['sigDevEn_test_pitch', 'cv_test_pitch', 'sigDevEn_test_acc', 'cv_test_acc'])\n",
    "subj_2_meta_data = [0, 0, 0, 0, 0, 0, 0, 1]\n",
    "subj_2_test_results['Metadata Results'] = subj_2_meta_data\n",
    "\n",
    "print(subj_2_test_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F.3) Balance metrics for subject 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 4)\n",
    "fig.subplots_adjust(hspace=.5, wspace=.6)\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "axs = axs.ravel()\n",
    "i = 1\n",
    "for start, stop in zip(sub3_start, sub3_stop):\n",
    "\n",
    "    axs[i - 1].plot(sig_pitch_weig_filt[int(start):int(stop)],color='mediumblue')\n",
    "    axs[i - 1].twinx().plot(sig_totMag_weig_filt[int(start):int(stop)],color='orange')\n",
    "    axs[i - 1].set_title('test: ' + str(i))\n",
    "    axs[i - 1].set_ylim(-20, 60)\n",
    "    i = i + 1\n",
    "    \n",
    "axs[0].set_ylabel('Pitch (deg.) & Acc Mag(m^2/s)')\n",
    "axs[4].set_ylabel('Pitch (deg.) & Acc Mag(m^2/s)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_test_track = []\n",
    "sigDevEn_test_track = []\n",
    "\n",
    "cv_test_track_acc = []\n",
    "sigDevEn_test_acc_track = []\n",
    "for start, stop in zip(sub3_start, sub3_stop):\n",
    "    # for each test \n",
    "    # look at coefficient of variation - lower is better\n",
    "    # look at signal energy - lower is better\n",
    "    test_data = sig_pitch_weig_filt[int(start):int(stop)]\n",
    "    test_data_acc = sig_totMag_weig_filt[int(start):int(stop)]\n",
    "\n",
    "    cv_test_data = np.std(test_data) / np.mean(test_data)\n",
    "    sigDevEn_test_data = np.sum((test_data - np.mean(test_data)) ** 2) / len(test_data)\n",
    "\n",
    "    cv_test_track.append(cv_test_data)\n",
    "    sigDevEn_test_track.append(sigDevEn_test_data)\n",
    "    \n",
    "    cv_test_data_acc = np.std(test_data_acc) / np.mean(test_data_acc)\n",
    "    sigDevEn_test_data_acc = np.sum((test_data_acc - np.mean(test_data_acc)) ** 2) / len(test_data_acc)\n",
    "\n",
    "    cv_test_track_acc.append(cv_test_data_acc)\n",
    "    sigDevEn_test_acc_track.append(sigDevEn_test_data_acc)\n",
    "\n",
    "subj_3_data = np.vstack((sigDevEn_test_track, cv_test_track, cv_test_track_acc, sigDevEn_test_acc_track)).T\n",
    "subj_3_test_results = pd.DataFrame(subj_3_data,\n",
    "                                   columns=['sigDevEn_test_pitch', 'cv_test_pitch', 'sigDevEn_test_acc', 'cv_test_acc'])\n",
    "subj_3_meta_data = [0, 0, 0, 0, 0, 1, 0, 0]\n",
    "subj_3_test_results['Metadata Results'] = subj_3_meta_data\n",
    "\n",
    "print(subj_3_test_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G) Ranking performance\n",
    "There are lots of ways to rank performance. I've calculated some interesting statistics from the pitch and acceleration signals. It's hard to do this in a fair way. Because I'm nearly out of time, it might be best to just assume that a 'pass' is '0' and a 'fail' is a '1'. Then use the rank function and do a bunch of rank and sums to skew the difficulty of a test to favor the metadata results (i.e., make sure the failed tests are ranked low). The lower the ranking the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the rankings for each column\n",
    "subj_1_ranked = subj_1_test_results.rank()\n",
    "\n",
    "# add a ranked column as an additional column \n",
    "# lowest score is best. \n",
    "subj_1_ranked['sum rank'] = subj_1_ranked.sum(axis=1).rank()\n",
    "# print(subj_1_ranked)\n",
    "\n",
    "subj_2_ranked = subj_2_test_results.rank()\n",
    "subj_2_ranked['sum rank'] = subj_2_ranked.sum(axis=1).rank()\n",
    "# print(subj_2_ranked)\n",
    "\n",
    "subj_3_ranked = subj_3_test_results.rank()\n",
    "subj_3_ranked['sum rank'] = subj_3_ranked.sum(axis=1).rank()\n",
    "# print(subj_3_ranked)\n",
    "\n",
    "# then create a new matrix with just the sum ranks for each\n",
    "total_ranking = np.vstack((subj_1_ranked['sum rank'], subj_2_ranked['sum rank'], subj_3_ranked['sum rank'])).T\n",
    "\n",
    "total_ranking_df = pd.DataFrame(total_ranking, columns=['subj_1_sum_rank', 'subj_2_sum_rank', 'subj_3_sum_rank'])\n",
    "\n",
    "total_ranking_df['final_ranking'] = total_ranking_df.sum(axis=1).rank()\n",
    "print(total_ranking_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
